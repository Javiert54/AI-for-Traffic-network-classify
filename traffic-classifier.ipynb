{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install skimpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from skimpy import skim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('setup.json', 'r') as f:\n",
    "    SETUP_JSON = json.load(f)\n",
    "BASE_DIR = os.getcwd() # Base directory for the project,\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"datasets\") # Input directory,\n",
    "NA_VALUES = ['N/a', 'na', 'Na', 'NA', 'NAN', 'Nan', 'NaN', np.nan] # Consistent NA values,\n",
    "EXPECTED_DTYPES = None\n",
    "CHUNK_SIZE = SETUP_JSON['chunk_size'] # Chunk size for reading CSV files,\n",
    "SAMPLE_FRACTION = SETUP_JSON['sample_fraction'] # Fraction of data to sample,\n",
    "LABELS = SETUP_JSON['labels'] # Labels for the dataset\n",
    "final_dataframe = pd.DataFrame() # Initialize an empty DataFrame for final output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza previa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "def get_majority_dtype(series):\n",
    "    \"\"\"\n",
    "    Determina el tipo de dato mayoritario en una columna de un DataFrame.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): Columna del DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        type: Tipo de dato mayoritario o None si la serie está vacía.\n",
    "    \"\"\"\n",
    "    dtype_counts = series.map(type).value_counts()\n",
    "    return dtype_counts.idxmax() if not dtype_counts.empty else None\n",
    "\n",
    "def clean_by_majority_type_chunked(input_filepath, output_filepath, chunksize=10000):\n",
    "    \"\"\"\n",
    "    Limpia un archivo CSV en bloques de manera eficiente, intentando convertir valores antes de eliminarlos.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): Ruta del archivo CSV original.\n",
    "        output_filepath (str): Ruta del archivo donde guardar el CSV limpio.\n",
    "        chunksize (int): Tamaño del bloque para leer el archivo en partes.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_chunks = []\n",
    "\n",
    "    for chunk in pd.read_csv(input_filepath, chunksize=chunksize, low_memory=False):\n",
    "        chunk = chunk.convert_dtypes()  # Conversión automática de tipos en Pandas\n",
    "        \n",
    "        for col in chunk.columns:\n",
    "            majority_dtype = get_majority_dtype(chunk[col])\n",
    "            if majority_dtype:\n",
    "                try:\n",
    "                    chunk[col] = chunk[col].astype(majority_dtype)\n",
    "                    print(f\"\\nColumna '{col}' convertida a {majority_dtype}.\")\n",
    "                except ValueError:\n",
    "                    chunk[col] = pd.to_numeric(chunk[col], errors='coerce')  # Intentar conversión numérica\n",
    "                    print(f\"\\nAlgunos valores en '{col}' no pudieron convertirse. Se reemplazaron con NaN.\")\n",
    "\n",
    "                # Eliminar valores que no pudieron convertirse\n",
    "                chunk = chunk.dropna(subset=[col])\n",
    "\n",
    "        cleaned_chunks.append(chunk)\n",
    "\n",
    "    df_final = pd.concat(cleaned_chunks, ignore_index=True)\n",
    "\n",
    "    # Guardar el dataset limpio\n",
    "    df_final.to_csv(output_filepath, index=False)\n",
    "    print(f\"\\nDataset limpio guardado en: '{output_filepath}'\")\n",
    "\n",
    "# Configuración de directorios\n",
    "BASE_DIR = os.getcwd()\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"datasets\")\n",
    "csv_files = glob(os.path.join(DATASET_DIR, \"*.csv\"))\n",
    "\n",
    "# Procesar cada archivo CSV\n",
    "for file in csv_files:\n",
    "    clean_by_majority_type_chunked(file, file)  # Procesar archivo en chunks y sobrescribirlo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupar por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def contar_paquetes_por_secuencia(df, columna_labels='Label'):\n",
    "    \"\"\"\n",
    "    Cuenta el número de paquetes en cada secuencia consecutiva de etiquetas.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): El DataFrame que contiene los datos de los paquetes.\n",
    "        columna_labels (str): El nombre de la columna que contiene las etiquetas\n",
    "                              que definen las secuencias.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Un DataFrame con las columnas 'Labels' (la etiqueta de la secuencia),\n",
    "                      'count' (el número de paquetes en esa secuencia), y\n",
    "                      'sequence_id' (un identificador único para cada secuencia).\n",
    "                      Las secuencias están en el orden en que aparecen.\n",
    "    \"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        raise TypeError(\"La entrada 'df' debe ser un DataFrame de pandas.\")\n",
    "    if columna_labels not in df.columns:\n",
    "        raise ValueError(f\"La columna '{columna_labels}' no se encuentra en el DataFrame.\")\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[columna_labels, 'count', 'sequence_id'])\n",
    "\n",
    "    # Identificar dónde cambia la etiqueta para marcar el inicio de una nueva secuencia.\n",
    "    # df[columna_labels].shift() desplaza los valores de la columna una posición hacia abajo.\n",
    "    # La primera etiqueta siempre se considera el inicio de una nueva secuencia (manejo de NaN de shift()).\n",
    "    inicio_nueva_secuencia = (df[columna_labels] != df[columna_labels].shift())\n",
    "\n",
    "    # Asignar un ID único a cada secuencia usando cumsum() sobre la serie booleana.\n",
    "    # Cada vez que 'inicio_nueva_secuencia' es True, el acumulado aumenta, creando un nuevo ID.\n",
    "    id_secuencia = inicio_nueva_secuencia.cumsum()\n",
    "\n",
    "    # Agrupar por el ID de secuencia y la etiqueta, luego contar el tamaño de cada grupo.\n",
    "    # Esto nos da el conteo de paquetes para cada secuencia.\n",
    "    conteo_secuencias = df.groupby([id_secuencia, df[columna_labels]], sort=False).size()\n",
    "    conteo_secuencias = conteo_secuencias.reset_index(name='count')\n",
    "\n",
    "    # Renombrar la columna del ID de secuencia para mayor claridad.\n",
    "    conteo_secuencias.rename(columns={columna_labels: 'Labels', columna_labels + '_x': 'Labels_temp'}, inplace=True)\n",
    "    # El nombre de la columna de id_secuencia es el nombre de la columna_labels original\n",
    "    # después del groupby, así que lo renombramos a 'sequence_id'.\n",
    "    conteo_secuencias.rename(columns={conteo_secuencias.columns[0]: 'sequence_id'}, inplace=True)\n",
    "\n",
    "\n",
    "    return conteo_secuencias[['sequence_id', 'Labels', 'count']]\n",
    "\n",
    "df = pd.read_csv('datasets\\\\02-14-2018.csv')\n",
    "# Aplicar la función\n",
    "resultados = contar_paquetes_por_secuencia(df, 'Label')\n",
    "\n",
    "print(\"Conteo de paquetes por secuencia:\")\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chunk_overall = True # Flag to control header writing in the output file\n",
    "\n",
    "csv_files = glob(os.path.join(DATASET_DIR, \"*.csv\")) # Find all CSV files\n",
    "file_count = len(csv_files)\n",
    "print(f\"Found {file_count} files to process.\")\n",
    "\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found. Exiting.\")\n",
    "else:\n",
    "    # Iterate through each found CSV file\n",
    "    for i, file_path_str in enumerate(csv_files):\n",
    "        file_path = Path(file_path_str)\n",
    "        print(f\"\\nProcessing file {i+1}/{file_count}: {file_path.name}...\")\n",
    "\n",
    "        file_df = pd.read_csv(file_path, sep=',', low_memory=False, na_values=NA_VALUES, dtype=EXPECTED_DTYPES)\n",
    "        file_df[file_df['clase'] == 'B']\n",
    "        total_rows = sum(1 for _ in open(file_path, 'r')) - 1  # Subtract 1 for the header row\n",
    "        sample_size = max(1, int(total_rows * SAMPLE_FRACTION))  # Ensure at least one row is sampled\n",
    "        sample_df = pd.read_csv(file_path, sep=',', low_memory=False, na_values=NA_VALUES, nrows=sample_size, dtype=EXPECTED_DTYPES)\n",
    "        print(f\"  Sample of {sample_size} rows taken from {file_path.name} (shape: {sample_df.shape})...\")\n",
    "\n",
    "    print(f\"\\nFinished processing all files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_in_chunks(input_filepath):\n",
    "    \"\"\"\n",
    "    Procesa un archivo CSV en chunks y realiza operaciones en cada chunk.\n",
    "\n",
    "    Args:\n",
    "        input_filepath (str): Ruta del archivo CSV a procesar.\n",
    "        chunksize (int): Tamaño del chunk para leer el archivo en partes.\n",
    "    \"\"\"\n",
    "    chunk_list = []  # Lista para almacenar los chunks procesados\n",
    "\n",
    "    for chunk in pd.read_csv(input_filepath, chunksize=CHUNK_SIZE, na_values=NA_VALUES, dtype=EXPECTED_DTYPES):\n",
    "        # Realiza operaciones en cada chunk aquí\n",
    "        chunk_cleaned = chunk.dropna()  # Ejemplo: eliminar filas con valores NaN\n",
    "        chunk_list.append(chunk_cleaned)\n",
    "\n",
    "    # Combina todos los chunks procesados en un DataFrame final\n",
    "    final_df = pd.concat(chunk_list, ignore_index=True)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_chunk_overall = True # Flag to control header writing in the output file\n",
    "\n",
    "csv_files = glob(os.path.join(DATASET_DIR, \"*.csv\")) # Find all CSV files\n",
    "file_count = len(csv_files)\n",
    "label_groups = {}\n",
    "print(f\"Found {file_count} files to process.\")\n",
    "\n",
    "# Iterate through each found CSV file\n",
    "for i, file_path_str in enumerate(csv_files):\n",
    "    file_path = Path(file_path_str)\n",
    "    print(f\"\\nProcessing file {i+1}/{file_count}: {file_path.name}...\")\n",
    "\n",
    "    df = process_csv_in_chunks(file_path)\n",
    "    for label in LABELS:\n",
    "        label_groups[label] = df[df['Label'] == label]\n",
    "print(f\"\\nFinished processing all files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dimensiones del dataframe final: {final_dataframe.shape}\")\n",
    "print(f\"Diferentes etiquetas encontradas en el dataframe: {final_dataframe['Label'].unique()}\") # Display unique labels in the final DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skim(final_dataframe[:100000]) # Display a summary of the first 100,000 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

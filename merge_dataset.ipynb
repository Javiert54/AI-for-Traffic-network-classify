{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('setup.json', 'r') as f:\n",
    "    SETUP_JSON = json.load(f)\n",
    "DATASETS_PATH = SETUP_JSON['datasets_path'] # Path to the datasets,\n",
    "DATASETS_FOLDER = os.path.join(os.getcwd(), DATASETS_PATH) # Folder containing the datasets,\n",
    "DATASETS = glob.glob(os.path.join(DATASETS_FOLDER, '*.csv')) # List of datasets\n",
    "OUTPUT_CSV = SETUP_JSON['output_csv'] # Output CSV file\n",
    "OUTPUT_PARQUET = SETUP_JSON['output_parquet'] # Output CSV file\n",
    "N_ROWS = SETUP_JSON['n_rows']\n",
    "HEADER = SETUP_JSON['header']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de los datasets a combinar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener todos los encabezados diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('443', '6', '28/02/2018 08:22:13', '94658', '6.1', '7', '708', '3718', '387', '0', '118', '159.2846508613', '1460', '0.1', '531.1428571429', '673.1182235367', '46757.8017705846', '137.3365167234', '7888.1666666667', '11130.0425943262', '24325', '0.2', '72880', '14576', '12590.3839695221', '24385', '363', '72178', '12029.6666666667', '13189.2575176416', '24718', '0.3', '0.4', '0.5', '0.6', '0.7', '132', '152', '63.3860846416', '73.9504320818', '0.8', '1460.1', '316.1428571429', '519.2058813734', '269574.747252747', '0.9', '0.10', '1', '1.1', '0.11', '0.12', '0.13', '1.2', '1.3', '340.4615384615', '118.1', '531.1428571429.1', '0.14', '0.15', '0.16', '0.17', '0.18', '0.19', '6.2', '708.1', '7.1', '3718.1', '8192', '7484', '3', '20', '0.20', '0.21', '0.22', '0.23', '0.24', '0.25', '0.26', '0.27', 'Benign')\n",
      "('0', '0.1', '16/02/2018 08:27:23', '112640768', '3', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '0.10', '0.11', '0.12', '0.13', '0.026633341', '56300000', '138.5929291', '56300000.1', '56300000.2', '113000000', '56300000.3', '138.5929291.1', '56300000.4', '56300000.5', '0.14', '0.15', '0.16', '0.17', '0.18', '0.19', '0.20', '0.21', '0.22', '0.23', '0.24', '0.026633341.1', '0.25', '0.26', '0.27', '0.28', '0.29', '0.30', '0.31', '0.32', '0.33', '0.34', '0.35', '0.36', '0.37', '0.38', '0.39', '0.40', '0.41', '0.42', '0.43', '0.44', '0.45', '0.46', '0.47', '0.48', '3.1', '0.49', '0.50', '0.51', '-1', '-1.1', '0.52', '0.53', '0.54', '0.55', '0.56', '0.57', '56300000.6', '138.5929291.2', '56300000.7', '56300000.8', 'Benign')\n",
      "('Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label')\n",
      "('22', '6', '23/02/2018 08:18:29', '1532698', '11', '11.1', '1179', '1969', '648', '0', '107.1818181818', '196.2451620712', '976', '0.1', '179', '364.1864906885', '2053.894504984', '14.3537735418', '72985.619047619', '97519.1630344909', '207592', '11.2', '1532698.1', '153269.8', '106658.503762449', '246403', '20', '1325840', '132584', '106034.780827383', '247549', '67', '0.2', '0.3', '0.4', '0.5', '360', '360.1', '7.1768867709', '7.1768867709.1', '0.6', '976.1', '136.8695652174', '282.7939025233', '79972.3913043478', '0.7', '0.8', '0.9', '1', '0.10', '0.11', '0.12', '0.13', '1.1', '143.0909090909', '107.1818181818.1', '179.1', '0.14', '0.15', '0.16', '0.17', '0.18', '0.19', '11.3', '1179.1', '11.4', '1969.1', '29200', '230', '7', '32', '0.20', '0.21', '0.22', '0.23', '0.24', '0.25', '0.26', '0.27', 'Benign')\n",
      "('80', '6', '21/02/2018 08:33:25', '37953', '5', '3', '135', '127', '135.1', '0', '27', '60.37383539', '127.1', '0.1', '42.33333333', '73.32348419', '6903.275103', '210.7870261', '5421.857143', '5403.580246', '12099', '23', '37953.1', '9488.25', '3245.485108', '12382', '6013', '19960', '9980', '13546.75171', '19559', '401', '0.2', '0.3', '0.4', '0.5', '168', '104', '131.7418913', '79.04513477', '0.6', '135.2', '29.11111111', '57.80018262', '3340.861111', '0.7', '0.8', '0.9', '1', '0.10', '0.11', '0.12', '0.13', '0.14', '32.75', '27.1', '42.33333333.1', '0.15', '0.16', '0.17', '0.18', '0.19', '0.20', '5.1', '135.3', '3.1', '127.2', '29200', '219', '1.1', '32', '0.21', '0.22', '0.23', '0.24', '0.25', '0.26', '0.27', '0.28', 'Benign')\n",
      "('443', '6', '02/03/2018 08:47:38', '141385', '9', '7', '553', '3773', '202', '0', '61.44444444', '87.53443767', '1460', '0.1', '539', '655.4329358', '30597.30523', '113.1661775', '9425.666667', '19069.11685', '73403', '1', '141385.1', '17673125', '23965.32327', '73403.1', '22', '51417', '8569.5', '13036.89082', '31525', '1.1', '0.2', '0.3', '0.4', '0.5', '192', '152', '63.65597482', '49.51020264', '0.6', '1460.1', '254.4705882', '474.7129551', '225352.3897', '0.7', '0.8', '1.2', '1.3', '0.9', '0.10', '0.11', '1.4', '0.12', '270375', '61.44444444.1', '539.1', '0.13', '0.14', '0.15', '0.16', '0.17', '0.18', '9.1', '553.1', '7.1', '3773.1', '8192', '119', '4', '20', '0.19', '0.20', '0.21', '0.22', '0.23', '0.24', '0.25', '0.26', 'Benign')\n",
      "('0', '0.1', '15/02/2018 08:25:18', '112641158', '3', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '0.10', '0.11', '0.12', '0.13', '0.0266332489', '56320579', '704.2783540618', '56321077', '56320081', '112641158.1', '56320579.1', '704.2783540618.1', '56321077.1', '56320081.1', '0.14', '0.15', '0.16', '0.17', '0.18', '0.19', '0.20', '0.21', '0.22', '0.23', '0.24', '0.0266332489.1', '0.25', '0.26', '0.27', '0.28', '0.29', '0.30', '0.31', '0.32', '0.33', '0.34', '0.35', '0.36', '0.37', '0.38', '0.39', '0.40', '0.41', '0.42', '0.43', '0.44', '0.45', '0.46', '0.47', '0.48', '3.1', '0.49', '0.50', '0.51', '-1', '-1.1', '0.52', '0.53', '0.54', '0.55', '0.56', '0.57', '56320579.2', '704.2783540618.2', '56321077.2', '56320081.2', 'Benign')\n",
      "('Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label')\n",
      "('22', '6', '22/02/2018 08:26:03', '20553406', '10', '7', '1063', '1297', '744', '0', '106.3', '239.357496459', '976', '0.1', '185.2857142857', '363.3963466546', '114.8228181743', '0.8271135207', '1284587875', '4865569.08388705', '19526080', '12', '20553406.1', '2283711.77777778', '6467164.61703247', '19526080.1', '22.1', '782332', '130388.666666667', '126280.381324522', '245143', '2639', '0.2', '0.3', '0.4', '0.5', '212', '152', '0.4865373651', '0.3405761556', '0.6', '976.1', '131.1111111111', '281.9949705202', '79521.1633986928', '0.7', '0.8', '0.9', '1', '0.10', '0.11', '0.12', '0.13', '0.14', '138.8235294118', '106.3.1', '185.2857142857.1', '0.15', '0.16', '0.17', '0.18', '0.19', '0.20', '10.1', '1063.1', '7.1', '1297.1', '14600', '222', '4', '20', '1027304', '0.21', '1027304.1', '1027304.2', '19526080.2', '0.22', '19526080.3', '19526080.4', 'Benign')\n",
      "('0', '0.1', '01/03/2018 08:17:11', '115307855', '5', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '0.10', '0.11', '0.12', '0.13', '0.04336218', '28800000', '32400000', '61000000', '812396', '115000000', '28800000.1', '32400000.1', '61000000.1', '812396.1', '0.14', '0.15', '0.16', '0.17', '0.18', '0.19', '0.20', '0.21', '0.22', '0.23', '0.24', '0.04336218.1', '0.25', '0.26', '0.27', '0.28', '0.29', '0.30', '0.31', '0.32', '0.33', '0.34', '0.35', '0.36', '0.37', '0.38', '0.39', '0.40', '0.41', '0.42', '0.43', '0.44', '0.45', '0.46', '0.47', '0.48', '5.1', '0.49', '0.50', '0.51', '-1', '-1.1', '0.52', '0.53', '1812348', '0.54', '1812348.1', '1812348.2', '56700000', '6010057622', '61000000.2', '52500000', 'Benign')\n"
     ]
    }
   ],
   "source": [
    "unique_headers = set()\n",
    "for dataset in DATASETS:\n",
    "    df = pd.read_csv(dataset, nrows=1)\n",
    "    df_dtypes = df.dtypes.keys()\n",
    "    unique_headers.add(tuple(df_dtypes))\n",
    "for header in unique_headers:\n",
    "    print(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar los grupos de datasets por encabezados diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-28-2018.csv\n",
      "Número de columnas del grupo de datasets: 80\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-16-2018.csv\n",
      "Número de columnas del grupo de datasets: 80\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-20-2018.csv\n",
      "Número de columnas del grupo de datasets: 84\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-23-2018.csv\n",
      "Número de columnas del grupo de datasets: 80\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-21-2018.csv\n",
      "Número de columnas del grupo de datasets: 80\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-02-2018.csv\n",
      "Número de columnas del grupo de datasets: 80\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-15-2018.csv\n",
      "Número de columnas del grupo de datasets: 80\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-14-2018.csv\n",
      "Número de columnas del grupo de datasets: 80\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-22-2018.csv\n",
      "Número de columnas del grupo de datasets: 80\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-01-2018.csv\n",
      "Número de columnas del grupo de datasets: 80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets_by_header = {}\n",
    "for header in unique_headers:\n",
    "    datasets_group = []\n",
    "    for dataset in DATASETS:\n",
    "        df = pd.read_csv(dataset, nrows=1)\n",
    "        df_dtypes = df.dtypes.keys()\n",
    "        if tuple(df_dtypes) == header:\n",
    "            datasets_group.append(dataset)\n",
    "    datasets_by_header[header] = datasets_group\n",
    "    print(f\"Grupo de datasets:\")\n",
    "    for dataset in datasets_group:\n",
    "        print(dataset)\n",
    "    print(f\"Número de columnas del grupo de datasets: {len(header)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar los grupos de datasets por etiquetas diferentes encontradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-28-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'Infilteration', 'Label'}\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-16-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'DoS attacks-SlowHTTPTest', 'DoS attacks-Hulk', 'Label'}\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-20-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'DDoS attacks-LOIC-HTTP'}\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-23-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'Brute Force -Web', 'SQL Injection', 'Brute Force -XSS'}\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-21-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'DDOS attack-LOIC-UDP', 'DDOS attack-HOIC'}\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-02-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'Bot'}\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-15-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'DoS attacks-Slowloris', 'DoS attacks-GoldenEye'}\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-14-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'FTP-BruteForce', 'SSH-Bruteforce'}\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-22-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'Brute Force -Web', 'SQL Injection', 'Brute Force -XSS'}\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-01-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'Infilteration', 'Label'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets_by_num_labels = {}\n",
    "\n",
    "for datasets_group in datasets_by_header.values():\n",
    "    unique_labels = set()\n",
    "    for dataset in datasets_group:\n",
    "        df = pd.read_csv(dataset, dtype=str)\n",
    "        last_column = df.columns[-1]  # Obtiene el nombre de la última columna\n",
    "        for label in list(df[last_column]):  # Usa la última columna en lugar de \"Label\"\n",
    "            unique_labels.add(label)\n",
    "    datasets_by_num_labels[tuple(datasets_group)] = len(unique_labels)\n",
    "    print(f\"Grupo de datasets:\")\n",
    "    for dataset in datasets_group:\n",
    "        print(dataset)\n",
    "    print(f\"Etiquetas diferentes encontradas: \", unique_labels)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionar el grupo de datasets que abarca más etiquetas diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selección de datasets a procesar:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-22-2018.csv\n"
     ]
    }
   ],
   "source": [
    "for datasets_group, num_labels in datasets_by_num_labels.items():\n",
    "    if (num_labels) == max(datasets_by_num_labels.values()):\n",
    "        datasets_selected = datasets_group\n",
    "print(f\"Selección de datasets a procesar:\")\n",
    "for dataset in datasets_selected:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportar encabezado del grupo de datasets en formato JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar encabezado de los datasets a combinar\n",
    "header = pd.read_csv(datasets_selected[0], nrows=1).dtypes.to_dict()\n",
    "header = {col: str(dtype) for col, dtype in header.items()}\n",
    "# header.pop('Label')\n",
    "\n",
    "# Exportar el encabezado\n",
    "with open(HEADER, 'w') as f:\n",
    "    json.dump(header, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinar datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos todos los datasets seleccionados en un sólo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-22-2018.csv concatenado\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for dataset in datasets_selected:\n",
    "    current_df = pd.read_csv(dataset, nrows=N_ROWS, dtype=str)\n",
    "    df = pd.concat([df, current_df], ignore_index=True, sort=False)\n",
    "    print(f\"Dataset {dataset} concatenado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 80)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22                     object\n",
       "6                      object\n",
       "22/02/2018 08:26:03    object\n",
       "20553406               object\n",
       "10                     object\n",
       "                        ...  \n",
       "19526080.2             object\n",
       "0.22                   object\n",
       "19526080.3             object\n",
       "19526080.4             object\n",
       "Benign                 object\n",
       "Length: 80, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportación del dataframe en formato CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Dst Port  Protocol            Timestamp  Flow Duration  Tot Fwd Pkts  \\\n",
      "0           0.0       0.0  14/02/2018 08:31:01    112641719.0           3.0   \n",
      "1           NaN       NaN                  NaN            NaN           NaN   \n",
      "2           NaN       NaN                  NaN            NaN           NaN   \n",
      "3           NaN       NaN                  NaN            NaN           NaN   \n",
      "4           NaN       NaN                  NaN            NaN           NaN   \n",
      "...         ...       ...                  ...            ...           ...   \n",
      "79996       NaN       NaN                  NaN            NaN           NaN   \n",
      "79997       NaN       NaN                  NaN            NaN           NaN   \n",
      "79998       NaN       NaN                  NaN            NaN           NaN   \n",
      "79999       NaN       NaN                  NaN            NaN           NaN   \n",
      "80000       NaN       NaN                  NaN            NaN           NaN   \n",
      "\n",
      "       Tot Bwd Pkts  TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  \\\n",
      "0               0.0              0.0              0.0              0.0   \n",
      "1               NaN              NaN              NaN              NaN   \n",
      "2               NaN              NaN              NaN              NaN   \n",
      "3               NaN              NaN              NaN              NaN   \n",
      "4               NaN              NaN              NaN              NaN   \n",
      "...             ...              ...              ...              ...   \n",
      "79996           NaN              NaN              NaN              NaN   \n",
      "79997           NaN              NaN              NaN              NaN   \n",
      "79998           NaN              NaN              NaN              NaN   \n",
      "79999           NaN              NaN              NaN              NaN   \n",
      "80000           NaN              NaN              NaN              NaN   \n",
      "\n",
      "       Fwd Pkt Len Min  ...   20  1027304  0.21  1027304.1  1027304.2  \\\n",
      "0                  0.0  ...  NaN      NaN   NaN        NaN        NaN   \n",
      "1                  NaN  ...   20        0     0          0          0   \n",
      "2                  NaN  ...    8  4000203     0    4000203    4000203   \n",
      "3                  NaN  ...    8  4000189     0    4000189    4000189   \n",
      "4                  NaN  ...    8  4000554     0    4000554    4000554   \n",
      "...                ...  ...  ...      ...   ...        ...        ...   \n",
      "79996              NaN  ...    8        0     0          0          0   \n",
      "79997              NaN  ...    8        0     0          0          0   \n",
      "79998              NaN  ...   20        0     0          0          0   \n",
      "79999              NaN  ...   20        0     0          0          0   \n",
      "80000              NaN  ...    8        0     0          0          0   \n",
      "\n",
      "             19526080.2              0.22  19526080.3  19526080.4  Benign  \n",
      "0                   NaN               NaN         NaN         NaN     NaN  \n",
      "1                     0                 0           0           0  Benign  \n",
      "2      31915236.6666667  37927869.4859419    75584115     7200679  Benign  \n",
      "3      31915241.3333333  37927877.3079527    75584130     7200693  Benign  \n",
      "4           21370201.75  15281092.9995064    41990741     7200848  Benign  \n",
      "...                 ...               ...         ...         ...     ...  \n",
      "79996                 0                 0           0           0  Benign  \n",
      "79997                 0                 0           0           0  Benign  \n",
      "79998                 0                 0           0           0  Benign  \n",
      "79999                 0                 0           0           0  Benign  \n",
      "80000                 0                 0           0           0  Benign  \n",
      "\n",
      "[80001 rows x 160 columns]\n",
      "Archivo CSV guardado como merged_output.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>22</th>\n",
       "      <th>6</th>\n",
       "      <th>22/02/2018 08:26:03</th>\n",
       "      <th>20553406</th>\n",
       "      <th>10</th>\n",
       "      <th>7</th>\n",
       "      <th>1063</th>\n",
       "      <th>1297</th>\n",
       "      <th>744</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>1027304</th>\n",
       "      <th>0.21</th>\n",
       "      <th>1027304.1</th>\n",
       "      <th>1027304.2</th>\n",
       "      <th>19526080.2</th>\n",
       "      <th>0.22</th>\n",
       "      <th>19526080.3</th>\n",
       "      <th>19526080.4</th>\n",
       "      <th>Benign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34989</td>\n",
       "      <td>6</td>\n",
       "      <td>22/02/2018 08:26:24</td>\n",
       "      <td>790</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>848</td>\n",
       "      <td>0</td>\n",
       "      <td>848</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>500</td>\n",
       "      <td>17</td>\n",
       "      <td>22/02/2018 08:25:10</td>\n",
       "      <td>99745913</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>4000203</td>\n",
       "      <td>0</td>\n",
       "      <td>4000203</td>\n",
       "      <td>4000203</td>\n",
       "      <td>31915236.6666667</td>\n",
       "      <td>37927869.4859419</td>\n",
       "      <td>75584115</td>\n",
       "      <td>7200679</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>17</td>\n",
       "      <td>22/02/2018 08:25:10</td>\n",
       "      <td>99745913</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>4000189</td>\n",
       "      <td>0</td>\n",
       "      <td>4000189</td>\n",
       "      <td>4000189</td>\n",
       "      <td>31915241.3333333</td>\n",
       "      <td>37927877.3079527</td>\n",
       "      <td>75584130</td>\n",
       "      <td>7200693</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>17</td>\n",
       "      <td>22/02/2018 08:24:59</td>\n",
       "      <td>89481361</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>4000554</td>\n",
       "      <td>0</td>\n",
       "      <td>4000554</td>\n",
       "      <td>4000554</td>\n",
       "      <td>21370201.75</td>\n",
       "      <td>15281092.9995064</td>\n",
       "      <td>41990741</td>\n",
       "      <td>7200848</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>17</td>\n",
       "      <td>22/02/2018 08:24:59</td>\n",
       "      <td>89481358</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>4000553</td>\n",
       "      <td>0</td>\n",
       "      <td>4000553</td>\n",
       "      <td>4000553</td>\n",
       "      <td>21370201.25</td>\n",
       "      <td>15281092.1551763</td>\n",
       "      <td>41990740</td>\n",
       "      <td>7200849</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      22   6  22/02/2018 08:26:03  20553406 10  7  1063 1297  744    0  ...  \\\n",
       "0  34989   6  22/02/2018 08:26:24       790  2  0   848    0  848    0  ...   \n",
       "1    500  17  22/02/2018 08:25:10  99745913  5  0  2500    0  500  500  ...   \n",
       "2    500  17  22/02/2018 08:25:10  99745913  5  0  2500    0  500  500  ...   \n",
       "3    500  17  22/02/2018 08:24:59  89481361  6  0  3000    0  500  500  ...   \n",
       "4    500  17  22/02/2018 08:24:59  89481358  6  0  3000    0  500  500  ...   \n",
       "\n",
       "   20  1027304 0.21 1027304.1 1027304.2        19526080.2              0.22  \\\n",
       "0  20        0    0         0         0                 0                 0   \n",
       "1   8  4000203    0   4000203   4000203  31915236.6666667  37927869.4859419   \n",
       "2   8  4000189    0   4000189   4000189  31915241.3333333  37927877.3079527   \n",
       "3   8  4000554    0   4000554   4000554       21370201.75  15281092.9995064   \n",
       "4   8  4000553    0   4000553   4000553       21370201.25  15281092.1551763   \n",
       "\n",
       "  19526080.3 19526080.4  Benign  \n",
       "0          0          0  Benign  \n",
       "1   75584115    7200679  Benign  \n",
       "2   75584130    7200693  Benign  \n",
       "3   41990741    7200848  Benign  \n",
       "4   41990740    7200849  Benign  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Archivo CSV guardado como {OUTPUT_CSV}\")\n",
    "df.to_parquet(OUTPUT_PARQUET)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def find_datasets_with_same_header():\n",
    "    headers = {}\n",
    "    same_files = {}\n",
    "\n",
    "    for file in DATASETS:\n",
    "        with open(file, newline='', encoding='utf-8') as f:\n",
    "            header = tuple(next(csv.reader(f), None))\n",
    "            if header:\n",
    "                if header in headers:\n",
    "                    headers[header].append(file)\n",
    "                else:\n",
    "                    headers[header] = [file]\n",
    "    print(\"Archivos con headers distintos:\", tuple(headers.values())[1])\n",
    "    return tuple(headers.values())[0]\n",
    "\n",
    "DATASETS = find_datasets_with_same_header()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def mixed_types_columns(df):\n",
    "    \"\"\"\n",
    "    # Recorre todas las columnas del DataFrame y devuelve un diccionario\n",
    "    # con las columnas que contienen más de un tipo de dato.\n",
    "\n",
    "    # Args:\n",
    "    #     df (pd.DataFrame): El DataFrame a analizar.\n",
    "\n",
    "    # Returns:\n",
    "    #     dict: Un diccionario con nombres de columnas como claves,\n",
    "    #           y otro diccionario con los tipos de datos y sus cantidades como valores.\n",
    "    \"\"\"\n",
    "    mixed_types_columns = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        types = {}\n",
    "        for val in df[col]:\n",
    "            typeVal = type(val)\n",
    "            types[typeVal] = types.get(typeVal, 0) + 1\n",
    "\n",
    "        if len(types) > 1:\n",
    "            mixed_types_columns[col] = types\n",
    "\n",
    "    return mixed_types_columns\n",
    "\n",
    "df = pd.read_csv('merged_output.csv')\n",
    "mixed_types_columns = mixed_types_columns(df)\n",
    "for col, types in mixed_types_columns.items():\n",
    "    print(f\"Columna: {col}\")\n",
    "    for typeVal, quantity in types.items():\n",
    "        print(f\"  type: {typeVal.__name__}, Cantidad: {quantity}\")\n",
    "    print()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"columns_lengths = []\n",
    "\n",
    "# Obtener número de columnas por dataset\n",
    "for dataset in DATASETS:\n",
    "    columns_length = len(pd.read_csv(dataset, nrows=1).dtypes)\n",
    "    columns_lengths.append(columns_length)\n",
    "\n",
    "# Obtener grupo de características más largo\n",
    "for dataset in DATASETS:\n",
    "    df = pd.read_csv(dataset, nrows=1)\n",
    "    columns_length = len(df.dtypes)\n",
    "    if columns_length == max(columns_lengths):\n",
    "        X_columns = df.dtypes.to_dict()\n",
    "        if 'Label' in X_columns:\n",
    "            X_columns.pop('Label')\n",
    "print(len(X_columns))\n",
    "for column, type in X_columns.items():\n",
    "    print(column, type)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

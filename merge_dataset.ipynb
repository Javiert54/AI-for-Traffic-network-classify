{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('setup.json', 'r') as f:\n",
    "    SETUP_JSON = json.load(f)\n",
    "DATASETS_PATH = SETUP_JSON['datasets_path'] # Path to the datasets,\n",
    "DATASETS_FOLDER = os.path.join(os.getcwd(), DATASETS_PATH) # Folder containing the datasets,\n",
    "DATASETS = glob.glob(os.path.join(DATASETS_FOLDER, '*.csv')) # List of datasets\n",
    "OUTPUT_CSV = SETUP_JSON['output_csv'] # Output CSV file\n",
    "OUTPUT_PARQUET = SETUP_JSON['output_parquet'] # Output CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las columnas únicas: 84\n",
      "Columna: Dst Port, Tipo: int64\n",
      "Columna: Protocol, Tipo: int64\n",
      "Columna: Timestamp, Tipo: object\n",
      "Columna: Flow Duration, Tipo: int64\n",
      "Columna: Tot Fwd Pkts, Tipo: int64\n",
      "Columna: Tot Bwd Pkts, Tipo: int64\n",
      "Columna: TotLen Fwd Pkts, Tipo: int64\n",
      "Columna: TotLen Bwd Pkts, Tipo: int64\n",
      "Columna: Fwd Pkt Len Max, Tipo: int64\n",
      "Columna: Fwd Pkt Len Min, Tipo: int64\n",
      "Columna: Fwd Pkt Len Mean, Tipo: float64\n",
      "Columna: Fwd Pkt Len Std, Tipo: float64\n",
      "Columna: Bwd Pkt Len Max, Tipo: int64\n",
      "Columna: Bwd Pkt Len Min, Tipo: int64\n",
      "Columna: Bwd Pkt Len Mean, Tipo: int64\n",
      "Columna: Bwd Pkt Len Std, Tipo: float64\n",
      "Columna: Flow Byts/s, Tipo: float64\n",
      "Columna: Flow Pkts/s, Tipo: float64\n",
      "Columna: Flow IAT Mean, Tipo: float64\n",
      "Columna: Flow IAT Std, Tipo: float64\n",
      "Columna: Flow IAT Max, Tipo: int64\n",
      "Columna: Flow IAT Min, Tipo: int64\n",
      "Columna: Fwd IAT Tot, Tipo: int64\n",
      "Columna: Fwd IAT Mean, Tipo: float64\n",
      "Columna: Fwd IAT Std, Tipo: float64\n",
      "Columna: Fwd IAT Max, Tipo: int64\n",
      "Columna: Fwd IAT Min, Tipo: int64\n",
      "Columna: Bwd IAT Tot, Tipo: int64\n",
      "Columna: Bwd IAT Mean, Tipo: float64\n",
      "Columna: Bwd IAT Std, Tipo: float64\n",
      "Columna: Bwd IAT Max, Tipo: int64\n",
      "Columna: Bwd IAT Min, Tipo: int64\n",
      "Columna: Fwd PSH Flags, Tipo: int64\n",
      "Columna: Bwd PSH Flags, Tipo: int64\n",
      "Columna: Fwd URG Flags, Tipo: int64\n",
      "Columna: Bwd URG Flags, Tipo: int64\n",
      "Columna: Fwd Header Len, Tipo: int64\n",
      "Columna: Bwd Header Len, Tipo: int64\n",
      "Columna: Fwd Pkts/s, Tipo: float64\n",
      "Columna: Bwd Pkts/s, Tipo: float64\n",
      "Columna: Pkt Len Min, Tipo: int64\n",
      "Columna: Pkt Len Max, Tipo: int64\n",
      "Columna: Pkt Len Mean, Tipo: float64\n",
      "Columna: Pkt Len Std, Tipo: float64\n",
      "Columna: Pkt Len Var, Tipo: float64\n",
      "Columna: FIN Flag Cnt, Tipo: int64\n",
      "Columna: SYN Flag Cnt, Tipo: int64\n",
      "Columna: RST Flag Cnt, Tipo: int64\n",
      "Columna: PSH Flag Cnt, Tipo: int64\n",
      "Columna: ACK Flag Cnt, Tipo: int64\n",
      "Columna: URG Flag Cnt, Tipo: int64\n",
      "Columna: CWE Flag Count, Tipo: int64\n",
      "Columna: ECE Flag Cnt, Tipo: int64\n",
      "Columna: Down/Up Ratio, Tipo: int64\n",
      "Columna: Pkt Size Avg, Tipo: float64\n",
      "Columna: Fwd Seg Size Avg, Tipo: float64\n",
      "Columna: Bwd Seg Size Avg, Tipo: int64\n",
      "Columna: Fwd Byts/b Avg, Tipo: int64\n",
      "Columna: Fwd Pkts/b Avg, Tipo: int64\n",
      "Columna: Fwd Blk Rate Avg, Tipo: int64\n",
      "Columna: Bwd Byts/b Avg, Tipo: int64\n",
      "Columna: Bwd Pkts/b Avg, Tipo: int64\n",
      "Columna: Bwd Blk Rate Avg, Tipo: int64\n",
      "Columna: Subflow Fwd Pkts, Tipo: int64\n",
      "Columna: Subflow Fwd Byts, Tipo: int64\n",
      "Columna: Subflow Bwd Pkts, Tipo: int64\n",
      "Columna: Subflow Bwd Byts, Tipo: int64\n",
      "Columna: Init Fwd Win Byts, Tipo: int64\n",
      "Columna: Init Bwd Win Byts, Tipo: int64\n",
      "Columna: Fwd Act Data Pkts, Tipo: int64\n",
      "Columna: Fwd Seg Size Min, Tipo: int64\n",
      "Columna: Active Mean, Tipo: int64\n",
      "Columna: Active Std, Tipo: int64\n",
      "Columna: Active Max, Tipo: int64\n",
      "Columna: Active Min, Tipo: int64\n",
      "Columna: Idle Mean, Tipo: int64\n",
      "Columna: Idle Std, Tipo: int64\n",
      "Columna: Idle Max, Tipo: int64\n",
      "Columna: Idle Min, Tipo: int64\n",
      "Columna: Label, Tipo: object\n",
      "Columna: Flow ID, Tipo: object\n",
      "Columna: Src IP, Tipo: object\n",
      "Columna: Src Port, Tipo: int64\n",
      "Columna: Dst IP, Tipo: object\n"
     ]
    }
   ],
   "source": [
    "# Obtener todas las columnas únicas y sus tipos\n",
    "all_columns_dtypes = {}\n",
    "for dataset in DATASETS:\n",
    "    df_dtypes = pd.read_csv(dataset, nrows=1).dtypes\n",
    "    df_dtypes.pop(dataset.columns[-1]) # Eliminar la columna de la etiqueta\n",
    "    all_columns_dtypes.update(df_dtypes.to_dict())\n",
    "\n",
    "print(f\"Todas las columnas únicas: {len(all_columns_dtypes)}\")\n",
    "for column in all_columns_dtypes:\n",
    "    print(f\"Columna: {column}, Tipo: {all_columns_dtypes[column]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinar datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos todos los archivos CSV y guardamos el dataset en formato CSV (para analizarlo con Tableau) y en formato Parquet (para su tratamiento con Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir columnas faltantes\n",
    "def add_missing_columns(filename, df):\n",
    "    print(f\"Procesando {filename}: {df.shape}\")\n",
    "    for col in all_columns_dtypes.keys():\n",
    "        if col not in df.columns:\n",
    "            df.insert(loc=df.shape[1] - 1, column=col, value=pd.NA)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando dataframe: (1048575, 80)\n",
      "Procesando dataframe: (1048575, 80)\n",
      "Procesando dataframe: (1048575, 80)\n",
      "Procesando dataframe: (7948748, 84)\n",
      "Procesando dataframe: (1048575, 80)\n",
      "Procesando dataframe: (1048575, 80)\n",
      "Procesando dataframe: (1048575, 80)\n",
      "Procesando dataframe: (613104, 80)\n",
      "Procesando dataframe: (331125, 80)\n",
      "Procesando dataframe: (1048575, 80)\n"
     ]
    }
   ],
   "source": [
    "# Combinar los datasets\n",
    "df = pd.DataFrame()\n",
    "first_df = add_missing_columns(DATASETS[0], pd.read_csv(DATASETS[0], dtype=str))\n",
    "df = pd.concat([df, first_df], ignore_index=True, sort=False)\n",
    "for dataset in DATASETS[1:]:\n",
    "    next_df = add_missing_columns(dataset, pd.read_csv(dataset, dtype=str))\n",
    "    df = pd.concat([df, next_df], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16233002, 84)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('O')], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()  # Display DataFrame information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportación de archivos JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los tipos a strings\n",
    "serializable_dtypes = {col: str(dtype) for col, dtype in all_columns_dtypes.items()}\n",
    "\n",
    "# Guardar en JSON\n",
    "with open('dtypes.json', 'w') as f:\n",
    "    json.dump(serializable_dtypes, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las etiquetas únicas\n",
    "labels = {}\n",
    "for label, i in enumerate(df[-1].unique()):\n",
    "    labels[label] = i\n",
    "labels.pop(df.columns[-1])\n",
    "print(f\"Etiquetas únicas: {labels}\")\n",
    "\n",
    "# Guardar etiquetas en JSON\n",
    "with open('labels.json', 'w') as f:\n",
    "    json.dump(labels, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportación del dataframe en CSV y Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV guardado como merged_output.csv\n",
      "Archivo Parquet guardado como merged_output.parquet\n"
     ]
    }
   ],
   "source": [
    "# Guardar en CSV\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Archivo CSV guardado como {OUTPUT_CSV}\")\n",
    "\n",
    "# Guardar en Parquet\n",
    "df.to_parquet(OUTPUT_PARQUET, index=False)\n",
    "print(f\"Archivo Parquet guardado como {OUTPUT_PARQUET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_datasets_with_same_header():\n",
    "    headers = {}\n",
    "    same_files = {}\n",
    "\n",
    "    for file in DATASETS:\n",
    "        with open(file, newline='', encoding='utf-8') as f:\n",
    "            header = tuple(next(csv.reader(f), None))\n",
    "            if header:\n",
    "                if header in headers:\n",
    "                    headers[header].append(\"datasets\\\\\"+file)\n",
    "                else:\n",
    "                    headers[header] = [\"datasets\\\\\"+file]\n",
    "    print(\"Archivos con headers distintos:\", tuple(headers.values())[1])\n",
    "    return tuple(headers.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos con headers distintos: ['datasets\\\\c:\\\\Users\\\\isard\\\\Desktop\\\\AI-for-Traffic-network-classify\\\\datasets\\\\02-20-2018.csv']\n",
      "Archivos con el mismo encabezado: ['datasets\\\\c:\\\\Users\\\\isard\\\\Desktop\\\\AI-for-Traffic-network-classify\\\\datasets\\\\02-14-2018.csv', 'datasets\\\\c:\\\\Users\\\\isard\\\\Desktop\\\\AI-for-Traffic-network-classify\\\\datasets\\\\02-15-2018.csv', 'datasets\\\\c:\\\\Users\\\\isard\\\\Desktop\\\\AI-for-Traffic-network-classify\\\\datasets\\\\02-16-2018.csv', 'datasets\\\\c:\\\\Users\\\\isard\\\\Desktop\\\\AI-for-Traffic-network-classify\\\\datasets\\\\02-21-2018.csv', 'datasets\\\\c:\\\\Users\\\\isard\\\\Desktop\\\\AI-for-Traffic-network-classify\\\\datasets\\\\02-22-2018.csv', 'datasets\\\\c:\\\\Users\\\\isard\\\\Desktop\\\\AI-for-Traffic-network-classify\\\\datasets\\\\02-23-2018.csv', 'datasets\\\\c:\\\\Users\\\\isard\\\\Desktop\\\\AI-for-Traffic-network-classify\\\\datasets\\\\02-28-2018.csv', 'datasets\\\\c:\\\\Users\\\\isard\\\\Desktop\\\\AI-for-Traffic-network-classify\\\\datasets\\\\03-01-2018.csv', 'datasets\\\\c:\\\\Users\\\\isard\\\\Desktop\\\\AI-for-Traffic-network-classify\\\\datasets\\\\03-02-2018.csv']\n"
     ]
    }
   ],
   "source": [
    "csv_files = find_datasets_with_same_header()\n",
    "print(\"Archivos con el mismo encabezado:\", csv_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('setup.json', 'r') as f:\n",
    "    SETUP_JSON = json.load(f)\n",
    "DATASETS_PATH = SETUP_JSON['datasets_path'] # Ruta de la carpeta de los datasets,\n",
    "DATASETS_FOLDER = os.path.join(os.getcwd(), DATASETS_PATH) # Carpeta de los datasets,\n",
    "DATASETS = glob.glob(os.path.join(DATASETS_FOLDER, '*.csv')) # Lista de los datasets\n",
    "RAW_DATASET_CSV = SETUP_JSON['raw_dataset_csv'] # Fichero CSV de salida\n",
    "RAW_DATASET_PARQUET = SETUP_JSON['raw_dataset_parquet'] # Fichero Parquet de salida\n",
    "TAKE_FULL_DATASET = SETUP_JSON['take_full_dataset'] # Obtener el dataset completo o una muestra\n",
    "NA_VAL = SETUP_JSON['navalues'] # Valores a considerar como NaN\n",
    "balances = SETUP_JSON['balances'] # Balances a considerar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de los datasets a combinar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener todos los encabezados diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label')\n",
      "('Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label')\n"
     ]
    }
   ],
   "source": [
    "unique_headers = set()\n",
    "for dataset in DATASETS:\n",
    "    df_dtypes = pd.read_csv(dataset, nrows=1).dtypes.keys()\n",
    "    unique_headers.add(tuple(df_dtypes))\n",
    "for header in unique_headers:\n",
    "    print(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar los grupos de datasets por encabezados diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-14-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-15-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-16-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-21-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-22-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-23-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-28-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-01-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-02-2018.csv\n",
      "Número de columnas del grupo de datasets: 80\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-20-2018.csv\n",
      "Número de columnas del grupo de datasets: 84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets_by_header = {}\n",
    "for header in unique_headers:\n",
    "    datasets_group = []\n",
    "    for dataset in DATASETS:\n",
    "        df_dtypes = pd.read_csv(dataset, nrows=1).dtypes.keys()\n",
    "        if tuple(df_dtypes) == header:\n",
    "            datasets_group.append(dataset)\n",
    "    datasets_by_header[header] = datasets_group\n",
    "    print(f\"Grupo de datasets:\")\n",
    "    for dataset in datasets_group:\n",
    "        print(dataset)\n",
    "    print(f\"Número de columnas del grupo de datasets: {len(header)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener etiquetas diferentes encontradas en cada grupo de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-14-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-15-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-16-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-21-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-22-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-23-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-28-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-01-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-02-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Bot', 'DoS attacks-Hulk', 'DoS attacks-GoldenEye', 'Brute Force -Web', 'DDOS attack-HOIC', 'Infilteration', 'Label', 'SQL Injection', 'DoS attacks-Slowloris', 'Brute Force -XSS', 'DoS attacks-SlowHTTPTest', 'SSH-Bruteforce', 'Benign', 'FTP-BruteForce', 'DDOS attack-LOIC-UDP'}\n",
      "\n",
      "Grupo de datasets:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-20-2018.csv\n",
      "Etiquetas diferentes encontradas:  {'Benign', 'DDoS attacks-LOIC-HTTP'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets_by_num_labels = {}\n",
    "\n",
    "for datasets_group in datasets_by_header.values():\n",
    "    unique_labels = set()\n",
    "    for dataset in datasets_group:\n",
    "        all_labels = list(pd.read_csv(dataset, usecols=[\"Label\"])[\"Label\"])\n",
    "        for label in all_labels:\n",
    "            unique_labels.add(label)\n",
    "    datasets_by_num_labels[tuple(datasets_group)] = len(unique_labels)\n",
    "    print(f\"Grupo de datasets:\")\n",
    "    for dataset in datasets_group:\n",
    "        print(dataset)\n",
    "    print(f\"Etiquetas diferentes encontradas: \", unique_labels)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionar el grupo de datasets que abarca más etiquetas diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selección de datasets a procesar:\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-14-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-15-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-16-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-21-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-22-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-23-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-28-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-01-2018.csv\n",
      "c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-02-2018.csv\n"
     ]
    }
   ],
   "source": [
    "for datasets_group, num_labels in datasets_by_num_labels.items():\n",
    "    if (num_labels) == max(datasets_by_num_labels.values()):\n",
    "        datasets_selected = datasets_group\n",
    "print(f\"Selección de datasets a procesar:\")\n",
    "for dataset in datasets_selected:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinar datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unimos todos los datasets seleccionados en un sólo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset concatenado: c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-14-2018.csv, Dimensiones: (1048575, 80)\n",
      "Dataset concatenado: c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-15-2018.csv, Dimensiones: (1048575, 80)\n",
      "Dataset concatenado: c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-16-2018.csv, Dimensiones: (1048575, 80)\n",
      "Dataset concatenado: c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-21-2018.csv, Dimensiones: (1048575, 80)\n",
      "Dataset concatenado: c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-22-2018.csv, Dimensiones: (1048575, 80)\n",
      "Dataset concatenado: c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-23-2018.csv, Dimensiones: (1048575, 80)\n",
      "Dataset concatenado: c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\02-28-2018.csv, Dimensiones: (613104, 80)\n",
      "Dataset concatenado: c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-01-2018.csv, Dimensiones: (331125, 80)\n",
      "Dataset concatenado: c:\\Users\\isard\\Desktop\\AI-for-Traffic-network-classify\\datasets\\03-02-2018.csv, Dimensiones: (1048575, 80)\n"
     ]
    }
   ],
   "source": [
    "# Inicializar DataFrame vacío\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for dataset in datasets_selected:\n",
    "    current_df = pd.read_csv(dataset, sep=',', low_memory=False, na_values=NA_VAL) \n",
    "    df = pd.concat([df, current_df], ignore_index=True)\n",
    "    print(f\"Dataset concatenado: {dataset}, Dimensiones: {current_df.shape}\")\n",
    "\n",
    "# Convertir todas las columnas a string\n",
    "df = df.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportación del dataset bruto en formato Parquet y CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportar el dataset en Parquet para preprocesamiento, entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo parquet guardado como raw_dataset.parquet\n"
     ]
    }
   ],
   "source": [
    "# Guardar en Parquet\n",
    "df.to_parquet(RAW_DATASET_PARQUET, index=False)\n",
    "print(f\"Archivo Parquet guardado como {RAW_DATASET_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportar el dataset en CSV para análisis exploratorio con Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo CSV guardado como raw_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Guardar en CSV\n",
    "df.to_csv(RAW_DATASET_CSV, index=False)\n",
    "print(f\"Archivo CSV guardado como {RAW_DATASET_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "\n",
    "# Inicializar DataFrame vacío\n",
    "df = pd.DataFrame()\n",
    "\n",
    "if TAKE_FULL_DATASET:\n",
    "    for dataset in datasets_selected:\n",
    "        current_df = pd.read_csv(dataset, sep=',', low_memory=False, na_values=NA_VAL) \n",
    "\n",
    "        # Identificar secuencias originales dentro de cada dataset\n",
    "        current_df[\"Original_Sequence_ID\"] = (current_df[\"Label\"] != current_df[\"Label\"].shift()).cumsum()\n",
    "\n",
    "        df = pd.concat([df, current_df], ignore_index=True)\n",
    "        print(f\"Dataset concatenado: {dataset}, Dimensiones: {current_df.shape}\")\n",
    "\n",
    "else:\n",
    "    for dataset in datasets_selected:\n",
    "        current_df = pd.read_csv(dataset, sep=',', low_memory=False, na_values=NA_VAL)\n",
    "        \n",
    "        # Identificar secuencias originales dentro de cada dataset\n",
    "        current_df[\"Original_Sequence_ID\"] = (current_df[\"Label\"] != current_df[\"Label\"].shift()).cumsum()\n",
    "\n",
    "        # Agregar una columna auxiliar que cuenta la aparición de cada Label\n",
    "        current_df[\"Label_Count\"] = current_df.groupby([\"Label\", \"Original_Sequence_ID\"]).cumcount() + 1\n",
    "\n",
    "        # Filtrar solo las primeras N apariciones de cada Label\n",
    "        copy_df = current_df[current_df.apply(lambda row: row[\"Label_Count\"] <= balances.get(row[\"Label\"], 0), axis=1)]\n",
    "\n",
    "        # Eliminar columna auxiliar antes de concatenar\n",
    "        copy_df.drop(columns=[\"Label_Count\"], inplace=True)\n",
    "\n",
    "        df = pd.concat([df, copy_df], ignore_index=True)\n",
    "\n",
    "        print(f\"Dataset concatenado: {dataset}, Dimensiones: {copy_df.shape}\")\n",
    "\n",
    "# Crear columna Sequence_ID considerando cambios en Label y Original_Sequence_ID\n",
    "df[\"Sequence_ID\"] = (df[\"Label\"] != df[\"Label\"].shift()) | (df[\"Original_Sequence_ID\"] != df[\"Original_Sequence_ID\"].shift())\n",
    "df[\"Sequence_ID\"] = df[\"Sequence_ID\"].cumsum()\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
